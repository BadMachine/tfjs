
////////////////////////////////////////////////////////////////////////////////
// The root "tfweb" namespace.
declare interface TFWeb {
  tfweb: {
    /**
     * Sets the path to load all the WASM module files from.
     *
     * TFWeb will automatically load WASM module with the best performance based
     * on whether the current browser supports WebAssembly SIMD and
     * multi-threading.
     *
     * @param path The path to load WASM module files from.
     *     For relative path, use :
     *         'relative/path/' (relative to current url path) or
     *         '/relative/path' (relative to the domain root).
     *     For absolute path, use https://some-server.com/absolute/path/.
     */
    setWasmPath(path: string): void;
  };

  /**
   * The generic TFLite model runner class.
   */
  TFWebModelRunner: TFWebModelRunnerStatic;

  // TODO: add task libraries.
}

export declare let tfweb: TFWeb;

////////////////////////////////////////////////////////////////////////////////
// TFWebModelRunner

declare interface TFWebModelRunnerStatic {
  /**
   * The factory function to create a TFWebModelRunner instance.
   *
   * @param modelPath The path to load the TFLite model from.
   * @param options Available options.
   */
  create(modelPath: string, options: TFWebModelRunnerOptions):
      Promise<TFWebModelRunner>;
}

/**
 * The main TFWebModelRunner class interface.
 *
 * It is a wrapper around TFLite Interpreter. See
 * https://www.tensorflow.org/lite/guide/inference for more info about related
 * concepts.
 */
export declare interface TFWebModelRunner {
  /** Gets model inputs. */
  getInputs(): TFWebModelRunnerTensorInfo[];

  /** Gets model outputs. */
  getOutputs(): TFWebModelRunnerTensorInfo[];

  /**
   * Run inference.
   *
   * @return Whether the inference is successful or not.
   */
  infer(): boolean;

  /** Cleans up. */
  cleanUp(): void;
}

/** Options for TFWebModelRunner. */
export declare interface TFWebModelRunnerOptions {
  /**
   * Number of threads to use when running inference.
   *
   * Set this to -1 to allow TFLite runtime to automatically pick threads count
   * based on current environment.
   */
  numThreads: number;
}

/** Stores metadata for a TFLite tensor. */
export declare interface TFWebModelRunnerTensorInfo {
  /** The id of the tensor (generated by TFLite runtime). */
  id: number;

  /** TFLite data type. */
  dataType: 'int8'|'uint8'|'bool'|'int16'|'int32'|'uint32'|'float32'|'float64';

  /** The name of the TFLite tensor. */
  name: string;

  /** The shape of the tensor in string form, e.g. "2,3,5". */
  shape: string;

  /** Gets the direct access to the underlying buffer. */
  data(): Int8Array|Uint8Array|Int16Array|Int32Array|Uint32Array|Float32Array
      |Float64Array;
}
